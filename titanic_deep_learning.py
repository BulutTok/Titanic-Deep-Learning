# -*- coding: utf-8 -*-
"""Titanic Deep Learning

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1S8DgH89RrQYFhHddT5sATxYCW-I_nP4v

##Tackle the Titanic dataset with Deep Learning

The goal is to predict whether or not a passenger survived based on attributes such as their age, sex, passenger class, where they embarked and so on.

Let's fetch the data and load it:
"""

import os
import urllib.request

TITANIC_PATH = os.path.join("datasets", "titanic")
DOWNLOAD_URL = "https://raw.githubusercontent.com/ageron/handson-ml2/master/datasets/titanic/"

def fetch_titanic_data(url=DOWNLOAD_URL, path=TITANIC_PATH):
    if not os.path.isdir(path):
        os.makedirs(path)
    for filename in ("train.csv", "test.csv"):
        filepath = os.path.join(path, filename)
        if not os.path.isfile(filepath):
            print("Downloading", filename)
            urllib.request.urlretrieve(url + filename, filepath)

fetch_titanic_data()

import pandas as pd

def load_titanic_data(filename, titanic_path=TITANIC_PATH):
    csv_path = os.path.join(titanic_path, filename)
    return pd.read_csv(csv_path)

train_data = load_titanic_data("train.csv")
test_data = load_titanic_data("test.csv")

"""The data is already split into a training set and a test set. However, the test data does *not* contain the labels: your goal is to train the best model you can using the training data, then make your predictions on the test data and upload them to Kaggle to see your final score.

The attributes have the following meaning:
* **PassengerId**: a unique identifier for each passenger
* **Survived**: that's the target, 0 means the passenger did not survive, while 1 means he/she survived.
* **Pclass**: passenger class.
* **Name**, **Sex**, **Age**: self-explanatory
* **SibSp**: how many siblings & spouses of the passenger aboard the Titanic.
* **Parch**: how many children & parents of the passenger aboard the Titanic.
* **Ticket**: ticket id
* **Fare**: price paid (in pounds)
* **Cabin**: passenger's cabin number
* **Embarked**: where the passenger embarked the Titanic

Let's explicitly set the `PassengerId` column as the index column:
"""

train_data = train_data.set_index("PassengerId")
test_data = test_data.set_index("PassengerId")

"""## Preprocessing

Now let's build our preprocessing pipelines, starting with the pipeline for numerical attributes:
"""

from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler

num_pipeline = Pipeline([
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler())
    ])

"""Now we can build the pipeline for the categorical attributes:"""

from sklearn.preprocessing import OneHotEncoder

cat_pipeline = Pipeline([
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("cat_encoder", OneHotEncoder(sparse=False)),
    ])

"""Finally, let's join the numerical and categorical pipelines:"""

from sklearn.compose import ColumnTransformer

num_attribs = ["Age", "SibSp", "Parch", "Fare"]
cat_attribs = ["Pclass", "Sex", "Embarked"]

preprocess_pipeline = ColumnTransformer([
        ("num", num_pipeline, num_attribs),
        ("cat", cat_pipeline, cat_attribs),
    ])

"""Cool! Now we have a nice preprocessing pipeline that takes the raw data and outputs numerical input features that we can feed to any Machine Learning model we want."""

X_train = preprocess_pipeline.fit_transform(
    train_data[num_attribs + cat_attribs])
X_train

"""Let's not forget to get the labels:"""

y_train = train_data["Survived"]

X_test = preprocess_pipeline.transform(test_data[num_attribs + cat_attribs])

"""We are now ready to train a classifier with Deep Learning."""

!pip install tensorflow #It is a deep learning library, pytorch is another deep learning library

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from keras.callbacks import ModelCheckpoint

model=Sequential()
model.add(Dense(100,activation='relu'))
model.add(Dense(1,activation='sigmoid'))
model.compile(loss='binary_crossentropy',optimizer='adam',metrics='accuracy')

#fit the model
model.fit(X_train,y_train, epochs=40)

model.summary()

import numpy as np
submission_dl = []

# As the sample, Survival must be expressed as either 0 or 1.
# But the prediction value from deep learning, It expressed percentage(0~1).
# So if its value higher than 0.5, It is more likely to have lived.
# let it transfer to 1. else transfer to 0.
for i in range(len(model.predict(X_test))):
    if model.predict(X_test)[i][0] >= 0.5:
        submission_dl.append(1)
    else:
        submission_dl.append(0)


y_pred = np.array(submission_dl)

"""And now we could just build a CSV file with these predictions (respecting the format excepted by Kaggle), then upload it and hope for the best."""

result=test_data.index.values
result

df_result = pd.DataFrame(result,columns=['PassengerId'])
df_result["Survived"]=y_pred
df_result

df_result.shape

df_result.to_csv("submission_dl.csv",index=False)